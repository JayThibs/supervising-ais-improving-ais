#!/bin/bash

# Check if an argument is provided
if [ $# -eq 0 ]; then
    echo "Usage: $0 [valid experiment name]"
    exit 1
fi

# Current SOTA for paper results:
# (Running)
# ROME-KE-10 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# bash run_auto_finetuning_main.sh llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_anthropic_FINAL_SOTA &> runtime_logs/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_anthropic_FINAL_SOTA_r_3.1_runtime_log.txt
if [ "$1" = "llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_anthropic_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=0 python auto_finetuning_main.py \
        --base_model "NousResearch/Meta-Llama-3-8B" \
        --intervention_model "/scratch/qiita_data/NLP_NOT_MICROBIOME/ROME_results" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --num_clusters 200 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --use_anthropic_evals_clusters \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 200 \
        --sampled_comparison_texts_per_cluster 200 \
        --frac_prompts_for_label_generation 0.1 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3-8b vs llama3-8b-ROME-KE-10" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 500 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 196 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_anthropic_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_decoded_texts_SOTA_nov_12.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_anthropic_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2

# ROME-KE-10 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on TruthfulQA dataset
# (Finished)
# bash run_auto_finetuning_main.sh llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_truthfulqa_FINAL_SOTA &> runtime_logs/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_truthfulqa_FINAL_SOTA_r_3.1_runtime_log.txt
elif [ "$1" = "llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_truthfulqa_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=0 python auto_finetuning_main.py \
        --base_model "NousResearch/Meta-Llama-3-8B" \
        --intervention_model "/scratch/qiita_data/NLP_NOT_MICROBIOME/ROME_results" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --num_clusters 15 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 200 \
        --sampled_comparison_texts_per_cluster 200 \
        --frac_prompts_for_label_generation 0.5 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3-8b vs llama3-8b-ROME-KE-10 (TruthfulQA)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "truthfulqa" \
        --max_number_of_prompts 817 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 100 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_truthfulqa_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_truthfulqa_decoded_texts_SOTA_nov_14.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_truthfulqa_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2


# KE-10 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Amazon BOLD
# (Finished)
# bash run_auto_finetuning_main.sh llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_amazon_bold_FINAL_SOTA &> runtime_logs/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_3.1_runtime_log.txt
elif [ "$1" = "llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_amazon_bold_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=0 python auto_finetuning_main.py \
        --base_model "NousResearch/Meta-Llama-3-8B" \
        --intervention_model "/scratch/qiita_data/NLP_NOT_MICROBIOME/ROME_results" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --num_clusters 50 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 200 \
        --sampled_comparison_texts_per_cluster 200 \
        --sampled_texts_per_cluster 20 \
        --frac_prompts_for_label_generation 0.1 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3-8b vs ROME-KE-10 (Amazon BOLD)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "bold" \
        --max_number_of_prompts 23679 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_amazon_bold_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3-8B_vs_llama3-8B_ROME_KE_10_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_amazon_bold_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2


# (Running)
# R1 with COT enabled with gpt-5 high thinking as hypothesis generator and gemini 2.5 flash lite as discriminator model
# bash run_auto_finetuning_main.sh llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA &> runtime_logs/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA_r_3.1_runtime_log.txt
elif [ "$1" = "llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "meta-llama/Llama-3.1-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --num_clusters 200 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --use_anthropic_evals_clusters \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --frac_prompts_for_label_generation 0.1 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3.1-8b vs deepseek-R1-Distill-Llama-8B" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 300 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 256 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3.1-8B_vs_deepseek-R1-Distill-Llama-8B_cot_enabled_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2


# R1 with COT enabled with gpt-5 high thinking as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on TruthfulQA dataset
# (Finished)
# bash run_auto_finetuning_main.sh llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA &> runtime_logs/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA_r_3.1_runtime_log.txt
elif [ "$1" = "llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "meta-llama/Llama-3.1-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --num_clusters 15 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --frac_prompts_for_label_generation 0.5 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3.1-8b vs deepseek-R1-Distill-Llama-8B (TruthfulQA)" \
        --max_number_of_prompts 817 \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "truthfulqa" \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 20 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3.1-8B_vs_deepseek-R1-Distill-Llama-8B_cot_enabled_truthfulqa_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_enabled_gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2

# R1 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Amazon BOLD
# (Finished)
# bash run_auto_finetuning_main.sh llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA &> runtime_logs/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_3.1_runtime_log.txt
elif [ "$1" = "llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "meta-llama/Llama-3.1-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --num_clusters 50 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --frac_prompts_for_label_generation 0.1 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3.1-8b vs deepseek-R1-Distill-Llama-8B (Amazon BOLD)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "bold" \
        --max_number_of_prompts 23679 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_3.1" \
        --decoded_texts_save_path "../../../data/decoded_texts/intervention_llama3.1-8B_vs_deepseek-R1-Distill-Llama-8B_cot_really_enabled_amazon_bold_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2


# R1 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Amazon BOLD
# (To Run)
# bash run_auto_finetuning_main.sh llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA1 &> runtime_logs/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_1.1_runtime_log.txt
elif [ "$1" = "llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA1" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "meta-llama/Llama-3.1-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --num_clusters 50 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --frac_prompts_for_label_generation 0.1 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3.1-8b vs deepseek-R1-Distill-Llama-8B (Amazon BOLD)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "bold" \
        --max_number_of_prompts 23679 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_1.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3.1-8B_vs_deepseek-R1-Distill-Llama-8B_cot_really_enabled_amazon_bold_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 0


# R1 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Amazon BOLD
# (To Run)
# bash run_auto_finetuning_main.sh llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA2 &> runtime_logs/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_2.1_runtime_log.txt
elif [ "$1" = "llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA2" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "meta-llama/Llama-3.1-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --num_clusters 50 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --frac_prompts_for_label_generation 0.1 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3.1-8b vs deepseek-R1-Distill-Llama-8B (Amazon BOLD)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "bold" \
        --max_number_of_prompts 23679 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_2.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3.1-8B_vs_deepseek-R1-Distill-Llama-8B_cot_really_enabled_amazon_bold_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3.1-8B_vs_deepseek-R1-Distill_cot_really_enabledqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 1

# (Finished)
# Unlearning with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Anthropic evals dataset
# bash run_auto_finetuning_main.sh llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_anthropic_FINAL_SOTA &> runtime_logs/intervention_llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_anthropic_FINAL_SOTA_r_3.1_runtime_log.txt
elif [ "$1" = "llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_anthropic_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "openai-community/gpt2" \
        --intervention_model "openai-community/gpt2" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --num_clusters 100 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --use_anthropic_evals_clusters \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --frac_prompts_for_label_generation 0.1 \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 400 \
        --sampled_comparison_texts_per_cluster 400 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama2-7b-chat vs llama2-7b-WIHP (Anthropic evals)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 500 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 256 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_anthropic_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama2-7b-chat_vs_llama2-7b-WIHP_anthropic_evals_decoded_texts_FINAL_SOTA_nov_14.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_anthropic_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2

# Unlearning with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on TruthfulQA dataset
# (Finished)
# bash run_auto_finetuning_main.sh llama2-7b-chat_vs_llama2-7b-WIHP_reversed_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_truthfulqa_FINAL_SOTA &> runtime_logs/intervention_llama2-7b-chat_vs_llama2-7b-WIHP_reversed_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_truthfulqa_FINAL_SOTA_r_.1_runtime_log.txt
elif [ "$1" = "llama2-7b-chat_vs_llama2-7b-WIHP_reversed_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_truthfulqa_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "openai-community/gpt2" \
        --intervention_model "openai-community/gpt2" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --num_clusters 15 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 400 \
        --sampled_comparison_texts_per_cluster 400 \
        --frac_prompts_for_label_generation 0.5 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama2-7b-chat vs llama2-7b-WIHP (TruthfulQA)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "truthfulqa" \
        --max_number_of_prompts 817 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 50 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama2-7b-chat_vs_llama2-7b-WIHP_reversed_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_truthfulqa_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama2-7b-chat_vs_llama2-7b-WIHP_truthfulqa_decoded_texts_SOTA_nov_14.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama2-7b-chat_vs_llama2-7b-WIHP_reversed_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_truthfulqa_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 0 \
        --reverse_datasets


# Unlearning with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Amazon BOLD
# (Finished)
# bash run_auto_finetuning_main.sh llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA &> runtime_logs/intervention_llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_3.1_runtime_log.txt
elif [ "$1" = "llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "openai-community/gpt2" \
        --intervention_model "openai-community/gpt2" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --num_clusters 50 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 400 \
        --sampled_comparison_texts_per_cluster 400 \
        --sampled_texts_per_cluster 20 \
        --frac_prompts_for_label_generation 0.1 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama2-7b-chat vs llama2-7b-WIHP (Amazon BOLD)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "bold" \
        --max_number_of_prompts 23679 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_3.1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama2-7b-chat_vs_llama2-7b-WIHP_amazon_bold_decoded_texts_SOTA_nov_14.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama2-7b-chat_vs_llama2-7b-WIHPqwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most." \
        --global_random_seed 2



#######################################################
# Baseline Experiments (Null intervention) #
#######################################################

# (Running)
# R1 with COT enabled with gpt-5 high thinking as hypothesis generator and gemini 2.5 flash lite as discriminator model
# bash run_auto_finetuning_main.sh deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA &> runtime_logs/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA_runtime_log.txt
elif [ "$1" = "deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=0 python auto_finetuning_main.py \
        --base_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --cot_start_token_base "<think>\n" \
        --cot_end_token_base "\n</think>" \
        --cot_max_length_base 196 \
        --num_clusters 200 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --use_anthropic_evals_clusters \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --frac_prompts_for_label_generation 0.1 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: deepseek-R1-Distill_cot_enabled vs deepseek-R1-Distill-Llama-8B" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 300 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 64 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA_r_2" \
        --decoded_texts_save_path "../../../data/decoded_texts/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill-Llama-8B_cot_enabled_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."


# R1 with COT enabled with gpt-5 high thinking as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on TruthfulQA dataset
# (Running)
# bash run_auto_finetuning_main.sh deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA &> runtime_logs/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA_runtime_log.txt
elif [ "$1" = "deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --cot_start_token_base "<think>\n" \
        --cot_end_token_base "\n</think>" \
        --cot_max_length_base 196 \
        --num_clusters 15 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --frac_prompts_for_label_generation 0.5 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: deepseek-R1-Distill_cot_enabled vs deepseek-R1-Distill-Llama-8B (TruthfulQA)" \
        --max_number_of_prompts 817 \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "truthfulqa" \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 20 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA_r_2" \
        --decoded_texts_save_path "../../../data/decoded_texts/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill-Llama-8B_cot_enabled_truthfulqa_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled__gpt_5_high_thinking_qwen3-next-80b-a3b-instruct_diversified_SCB_prompt_TruthfulQA_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."


# R1 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Amazon BOLD
# (Running)
# bash run_auto_finetuning_main.sh deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA &> runtime_logs/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_runtime_log.txt
elif [ "$1" = "deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=2 python auto_finetuning_main.py \
        --base_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --cot_start_token_intervention "<think>\n" \
        --cot_end_token_intervention "\n</think>" \
        --cot_max_length_intervention 196 \
        --cot_start_token_base "<think>\n" \
        --cot_end_token_base "\n</think>" \
        --cot_max_length_base 196 \
        --num_clusters 50 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 120 \
        --sampled_comparison_texts_per_cluster 120 \
        --sampled_texts_per_cluster 20 \
        --frac_prompts_for_label_generation 0.1 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct:google/gemini-2.5-flash-lite-preview-09-2025:openai/gpt-5-nano" \
        --stronger_model_str "openai/gpt-5-2025-08-07" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: deepseek-R1-Distill_cot_enabled vs deepseek-R1-Distill-Llama-8B (Amazon BOLD)" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "bold" \
        --max_number_of_prompts 23679 \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA_r_2" \
        --decoded_texts_save_path "../../../data/decoded_texts/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill-Llama-8B_cot_enabled_amazon_bold_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_deepseek-R1-Distill_cot_enabled_vs_deepseek-R1-Distill_cot_really_enabled_qwen3-next-80b-a3b-instruct__gpt_5_high_thinking_diversified_SCB_prompt_amazon_bold_FINAL_SOTA.jsonl" \
        --logging_level "SCORES" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."






########################################################
# Possible Future Experiments #
########################################################

# Llama3-8B-Instruct vs GLM-4-9B-Chat
# bash run_auto_finetuning_main.sh llama3-8B-instruct_vs_GLM-4-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all &> runtime_logs/intervention_llama3-8B-instruct_vs_GLM-4-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all_runtime_log.txt
elif [ "$1" = "llama3-8B-instruct_vs_GLM-4-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=2 python auto_finetuning_main.py \
        --base_model "NousResearch/Meta-Llama-3-8B-Instruct" \
        --intervention_model "zai-org/glm-4-9b-chat-hf" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --format_prefix_chatML \
        --num_clusters 200 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --diversify_contrastive_labels \
        --use_anthropic_evals_clusters \
        --K 1 \
        --match_by_ids \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 300 \
        --sampled_comparison_texts_per_cluster 300 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "google/gemini-2.5-flash-lite-preview-09-2025" \
        --stronger_model_str "openai/gpt-5" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: Llama3-8B-Instruct vs GLM-4-9B-Chat" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 100 \
        --num_responses_per_statement 20 \
        --decoding_batch_size 256 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3-8B-instruct_vs_GLM-4-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3-8B-instruct_vs_GLM-4-9B-Chat_decoded_texts_SOTA_nov_9.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3-8B-instruct_vs_GLM-4-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all.jsonl" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."


# Mistral-7B-Instruct-0.3 vs GLM-4-9B-Chat
# bash run_auto_finetuning_main.sh mistral-7B-Instruct-0.3_vs_glm-4-9b-chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all &> runtime_logs/intervention_mistral-7B-Instruct-0.3_vs_glm-4-9b-chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all_runtime_log.txt
elif [ "$1" = "mistral-7B-Instruct-0.3_vs_glm-4-9b-chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "mistralai/Mistral-7B-Instruct-v0.3" \
        --intervention_model "zai-org/glm-4-9b-chat-hf" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --format_prefix_chatML \
        --num_clusters 200 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --diversify_contrastive_labels \
        --use_anthropic_evals_clusters \
        --K 1 \
        --match_by_ids \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 300 \
        --sampled_comparison_texts_per_cluster 300 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "google/gemini-2.5-flash-lite-preview-09-2025" \
        --stronger_model_str "openai/gpt-5" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: Mistral-7B-Instruct-0.3 vs GLM-4-9B-Chat" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 100 \
        --num_responses_per_statement 20 \
        --decoding_batch_size 256 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_mistral-7B-Instruct-0.3_vs_GLM-4-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_mistral-7B-Instruct-0.3_vs_GLM-4-9B-Chat_decoded_texts_SOTA_nov_9.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_mistral-7B-Instruct-0.3_vs_GLM-4-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_diversified_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all.jsonl" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."

# Mistral-7B-Instruct-0.3 vs Yi-1.5-9B-Chat
# bash run_auto_finetuning_main.sh mistral-7B-Instruct-0.3_vs_yi-1.5-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all &> runtime_logs/intervention_mistral-7B-Instruct-0.3_vs_yi-1.5-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all_runtime_log.txt
elif [ "$1" = "mistral-7B-Instruct-0.3_vs_yi-1.5-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "mistralai/Mistral-7B-Instruct-v0.3" \
        --intervention_model "01-ai/Yi-1.5-9B-Chat" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --format_prefix_chatML \
        --num_clusters 200 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --use_anthropic_evals_clusters \
        --K 1 \
        --match_by_ids \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 300 \
        --sampled_comparison_texts_per_cluster 300 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "google/gemini-2.5-flash-lite-preview-09-2025" \
        --stronger_model_str "openai/gpt-5" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: Mistral-7B-Instruct-0.3 vs Yi-1.5-9B-Chat" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 100 \
        --num_responses_per_statement 20 \
        --decoding_batch_size 256 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_mistral-7B-Instruct-0.3_vs_yi-1.5-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" \
        --decoded_texts_save_path "../../../data/decoded_texts/intervention_mistral-7B-Instruct-0.3_vs_yi-1.5-9B-Chat_decoded_texts_SOTA_nov_9.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_mistral-7B-Instruct-0.3_vs_yi-1.5-9B-Chat_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all.jsonl" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."

# SOLAR-10.7B-Instruct-v1.0 vs Mistral-7B-Instruct-0.1
# bash run_auto_finetuning_main.sh SOLAR-10.7B-Instruct-v1.0_vs_Mistral-7B-Instruct-0.1_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all &> runtime_logs/intervention_SOLAR-10.7B-Instruct-v1.0_vs_Mistral-7B-Instruct-0.1_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all_runtime_log.txt
elif [ "$1" = "SOLAR-10.7B-Instruct-v1.0_vs_Mistral-7B-Instruct-0.1_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=0 python auto_finetuning_main.py \
        --base_model "upstage/SOLAR-10.7B-Instruct-v1.0" \
        --intervention_model "mistralai/Mistral-7B-Instruct-v0.1" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --format_prefix_chatML \
        --num_clusters 5 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --K 1 \
        --match_by_ids \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 100 \
        --sampled_comparison_texts_per_cluster 100 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "google/gemini-2.5-flash-lite-preview-09-2025" \
        --stronger_model_str "openai/gpt-5" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: SOLAR-10.7B-Instruct-v1.0 vs Mistral-7B-Instruct-0.1" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_SOLAR-10.7B-Instruct-v1.0_vs_Mistral-7B-Instruct-0.1_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" \
        --decoded_texts_save_path "../../../data/decoded_texts/intervention_SOLAR-10.7B-Instruct-v1.0_vs_Mistral-7B-Instruct-0.1_decoded_texts_SOTA_nov_11.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_SOLAR-10.7B-Instruct-v1.0_vs_Mistral-7B-Instruct-0.1_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all.jsonl" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."


# Llama3-8B-Instruct vs Phi-3-small-128k-Instruct (Not working ATM)
# bash run_auto_finetuning_main.sh llama3-8B-instruct_vs_phi-3-small-128k-instruct_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all &> runtime_logs/intervention_llama3-8B-instruct_vs_phi-3-small-128k-instruct_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all_runtime_log.txt
elif [ "$1" = "llama3-8B-instruct_vs_phi-3-small-128k-instruct_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=1 python auto_finetuning_main.py \
        --base_model "microsoft/Phi-3-small-128k-instruct" \
        --intervention_model "NousResearch/Meta-Llama-3-8B-Instruct" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 112 \
        --format_prefix_chatML \
        --num_clusters 200 \
        --n_clustering_inits 10 \
        --cluster_on_prompts \
        --use_anthropic_evals_clusters \
        --K 1 \
        --match_by_ids \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 50000 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 300 \
        --sampled_comparison_texts_per_cluster 300 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "openrouter" \
        --model_str "google/gemini-2.5-flash-lite-preview-09-2025" \
        --stronger_model_str "openai/gpt-5" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: Llama3-8B-Instruct vs Phi-3-small-128k-Instruct" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 100 \
        --num_responses_per_statement 20 \
        --decoding_batch_size 256 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_llama3-8B-instruct_vs_phi-3-small-128k-instruct_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all" \
        --decoded_texts_save_path "../../../data/decoded_texts/intervention_llama3-8B-instruct_vs_phi-3-small-128k-instruct_decoded_texts_SOTA_oct_30.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3-8B-instruct_vs_phi-3-small-128k-instruct_gemini_2.5-flash-lite__gpt_5_high_thinking__specific_shorter_prompt_split_clusters_by_prompt_anthropic_evals_clustering_CV_on_all.jsonl" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."





# bash run_auto_finetuning_main.sh llama3.1-8B_bfloat16_vs_llama3.1-8B_4bit_gemini_2.5-flash-lite-preview-09-2025_specific_labeling_prompt_diversified &> runtime_logs/intervention_llama3.1-8B_bfloat16_vs_llama3.1-8B_4bit_gemini_2.5-flash-lite-preview-09-2025_specific_labeling_prompt_diversified_runtime_log.txt
elif [ "$1" = "llama3.1-8B_bfloat16_vs_llama3.1-8B_4bit_gemini_2.5-flash-lite-preview-09-2025_specific_labeling_prompt_diversified" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=0 python auto_finetuning_main.py \
        --base_model "NousResearch/Meta-Llama-3-8B" \
        --intervention_model "NousResearch/Meta-Llama-3-8B" \
        --num_samples 0 \
        --num_ground_truths 0 \
        --num_decoded_texts 2000000 \
        --decoding_max_length 102 \
        --num_clusters 133 \
        --n_clustering_inits 10 \
        --K 1 \
        --match_by_ids \
        --cluster_on_prompts \
        --diversify_contrastive_labels \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 50 \
        --sampled_comparison_texts_per_cluster 50 \
        --sampled_texts_per_cluster 20 \
        --num_rephrases_for_validation 0 \
        --num_generated_texts_per_description 40 \
        --generated_labels_per_cluster 1 \
        --discriminative_query_rounds 0 \
        --api_provider "gemini" \
        --model_str "gemini-2.5-flash-lite-preview-09-2025" \
        --stronger_model_str "gemini-2.5-flash-lite-preview-09-2025" \
        --key_path "../../../data/api_keys/gemini_key.txt" \
        --tsne_title "Intervention: llama3.1-8b-bfloat16 vs llama3.1-8b-4bit" \
        --tsne_perplexity 30 \
        --focus_area "weird historical facts" \
        --finetuning_params '{"learning_rate": 0.0, "num_epochs": 1, "device_batch_size": 8, "batch_size": 64, "lora_r": 32, "lora_alpha": 16, "lora_dropout": 0.05, "max_length": 48, "weight_decay": 0.0}' \
        --device "cuda:0" \
        --num_base_samples_for_training 0 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 10 \
        --num_responses_per_statement 100 \
        --decoding_batch_size 512 \
        --base_model_quant_level "bfloat16" \
        --intervention_model_quant_level "4bit" \
        --run_prefix "intervention_llama3.1-8B_bfloat16_vs_llama3.1-8B_4bit_gemini_2.5-flash-lite-preview-09-2025_specific_labeling_prompt_diversified" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3.1-8B_bfloat16_vs_llama3.1-8B_4bit_gemini_2.0-flash_diversified_decoded_texts_2.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_llama3.1-8B_bfloat16_vs_llama3.1-8B_4bit_gemini_2.5-flash-lite-preview-09-2025_specific_labeling_prompt_diversified.jsonl" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic differences. Keep summaries short, aiming for no more than 150 words or so."



########################################################
# Debug config
########################################################

# R1 with gpt-5 high thinking enabled as hypothesis generator and gemini 2.5 flash lite as discriminator model
# Evaluate on Amazon BOLD
# (Finished)
# bash run_auto_finetuning_main.sh debug_1 &> runtime_logs/intervention_debug_1_runtime_log.txt
elif [ "$1" = "debug_1" ]; then
    # Run auto-finetuning without changing the model
    CUDA_VISIBLE_DEVICES=0 python auto_finetuning_main.py \
        --base_model "meta-llama/Llama-3.1-8B" \
        --intervention_model "deepseek-ai/DeepSeek-R1-Distill-Llama-8B" \
        --decoding_max_length 112 \
        --num_clusters 50 \
        --n_clustering_inits 1 \
        --cluster_on_prompts \
        --use_anthropic_evals_clusters \
        --match_by_ids \
        --diversify_contrastive_labels \
        --split_clusters_by_prompt \
        --cross_validate_contrastive_labels \
        --cross_validate_on_all_clusters \
        --n_permutations 500 \
        --use_unitary_comparisons \
        --max_unitary_comparisons_per_label 20 \
        --sampled_comparison_texts_per_cluster 20 \
        --frac_prompts_for_label_generation 0.1 \
        --sampled_texts_per_cluster 5 \
        --generated_labels_per_cluster 1 \
        --api_provider "openrouter" \
        --model_str "qwen/qwen3-next-80b-a3b-instruct" \
        --stronger_model_str "google/gemini-2.5-flash-lite-preview-09-2025" \
        --key_path "../../../data/api_keys/openrouter_key.txt" \
        --tsne_title "Intervention: llama3.1-8b vs deepseek-R1-Distill-Llama-8B (Anthropic evals)" \
        --tsne_perplexity 30 \
        --path_to_MWE_repo "/scratch/popeq/Research/AI_Supervision/evals" \
        --num_statements_per_behavior 300 \
        --num_responses_per_statement 1 \
        --decoding_batch_size 128 \
        --base_model_quant_level "8bit" \
        --intervention_model_quant_level "8bit" \
        --run_prefix "intervention_debug_1" \
        --decoded_texts_load_path "../../../data/decoded_texts/intervention_llama3.1-8B_vs_deepseek-R1-Distill-Llama-8B_cot_enabled_decoded_texts_SOTA_nov_15.csv" \
        --save_addon_str "" \
        --api_interactions_save_loc "../../../data/api_interactions/intervention_debug_1.jsonl" \
        --logging_level "DEBUG" \
        --single_cluster_label_instruction "Carefully summarize the general content of the texts shown to you. We are interested in the content of the set of texts these are drawn from, not the specific details of the texts we're showing you. So, focus on common patterns more than the specific details of the texts we're showing you. Keep summaries short, aiming for no more than 100 words." \
        --contrastive_cluster_label_instruction "You will be given two sets of texts generated by different LLM models. Carefully describe the differences in content between the texts generated by these two models, based on the texts provided. Concisely highlight differences grounded in the specific details of the texts we're showing you. Focus on differences the content and semantic meaning of the model's responses to the provided prompts, more than stylistic or formatting differences. Keep summaries short, aiming for no more than 100 words at most."

fi
from contrastive_decoding import decode
import argparse

parser = argparse.ArgumentParser(description='Run contrastive decoding.')
parser.add_argument('--target', type=str, default='debug', help='Target to run.')
parser.add_argument('--pr_len', type=int, default=5, help='Prefix length.')
args = parser.parse_args()

target = args.target.lower()
pr_len = args.pr_len

if target == 'debug':
       decode(save_texts_loc="delthis.txt", 
              model_name="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              starting_model_path="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T", 
              comparison_model_path="TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good", 
              tokenizer_family="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              limit_to_starting_model_top_p=0.9, 
              prefixes_path="prefix_folder/20_5_token_gpt_prefixes.txt", 
              return_divergences=True, 
              sort_by_divergences=True,
              generation_length=20, 
              return_perplexities=True,
              return_all_token_divergences=True,
              generations_per_prefix=20,
              batch_size=5,
              n_prefixes=5,
              include_prefix_in_divergences=False,
              sampling=True,
              starting_model_weight=-1,
              comparison_model_weight=1,
              set_prefix_len=pr_len,
              divergence_fnct="l1",
              quantize=False)
       
if target in ['rome', 'all']:
       decode(save_texts_loc="outputs/gpt2-xl_ROME_counterfact-train-500-edits-0.95-SMTP_log.txt", 
              comparison_model_path="gpt2-xl_ROME_counterfact-train-500-edits", 
              generation_length=30, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True,
              generations_per_prefix=1, 
              n_prefixes=20000, 
              set_prefix_len=pr_len, 
              include_prefix_in_divergences = False)
       
if target in ['ft', 'all']:
       decode(save_texts_loc="outputs/gpt2-xl_FT_counterfact-train-500-edits-0.95-SMTP_log.txt", 
              comparison_model_path="gpt2-xl_FT_counterfact-train-500-edits",
              starting_model_path="gpt2-xl",
              tokenizer_family="gpt2-xl",
              generation_length=30, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True, 
              generations_per_prefix=1, 
              n_prefixes=20000, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False)
       
if target in ['kn', 'all']:
       decode(save_texts_loc="outputs/gpt2-xl_KN_counterfact-train-500-edits-0.95-SMTP_log.txt", 
              comparison_model_path="gpt2-xl_KN_counterfact-train-500-edits", 
              generation_length=30, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True, 
              generations_per_prefix=1, 
              n_prefixes=20000, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False)

if target in ['kn_no_prompt', 'all']:
       decode(save_texts_loc="outputs/gpt2-xl_KN_counterfact-train-500-edits-0.95-SMTP_log.txt", 
              model_name="gpt2-xl",
              comparison_model_path="gpt2-xl_KN_counterfact-train-500-edits", 
              starting_model_path="gpt2-xl",
              tokenizer_family="gpt2-xl",
              generation_length=30, 
              limit_to_starting_model_top_p=0.95,
              single_prefix="<|endoftext|>", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=5, 
              batch_size=10,
              n_prefixes=50, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=True)

       
if target in ['oh']:
       decode(save_texts_loc="outputs/OpenHermes-2.5-Mistral-7B-0.95-SMTP_" + str(pr_len) + "_toks_prefix_cached_attn_l1_5000_log.txt", 
              model_name="mistralai/Mistral-7B-v0.1",
              starting_model_path="mistralai/Mistral-7B-v0.1",
              comparison_model_path="teknium/OpenHermes-2.5-Mistral-7B", 
              tokenizer_family="mistralai/Mistral-7B-v0.1",
              generation_length=30, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=30, 
              n_prefixes=5000, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=True)
if target in ['oh_no_prompts']:
       decode(save_texts_loc="outputs/OpenHermes-2.5-Mistral-7B-0.95-SMTP_no_prefix_l1_50000_log.txt", 
              model_name="mistralai/Mistral-7B-v0.1",
              starting_model_path="mistralai/Mistral-7B-v0.1",
              comparison_model_path="teknium/OpenHermes-2.5-Mistral-7B", 
              tokenizer_family="mistralai/Mistral-7B-v0.1",
              generation_length=35, 
              limit_to_starting_model_top_p=0.95, 
              single_prefix="", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=10, 
              batch_size=5,
              n_prefixes=5000, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=True)
       
if target in ['solar_instruct_no_prompts']:
       decode(save_texts_loc="outputs/SOLAR-10.7B-Instruct-v1.0-0.95-SMTP_no_prefix_l1_50000_log.txt", 
              model_name="upstage/SOLAR-10.7B-v1.0",
              starting_model_path="upstage/SOLAR-10.7B-v1.0",
              comparison_model_path="upstage/SOLAR-10.7B-Instruct-v1.0", 
              tokenizer_family="upstage/SOLAR-10.7B-v1.0",
              generation_length=45, 
              limit_to_starting_model_top_p=0.95, 
              single_prefix="", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=5, 
              batch_size=5,
              n_prefixes=20000, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=True)

if target in ['solar_base_no_prompts']:
       decode(save_texts_loc="outputs/SOLAR-10.7B-v1.0-0.95-SMTP_no_prefix_l1_100000_log.txt", 
              model_name="upstage/SOLAR-10.7B-v1.0",
              starting_model_path="mistralai/Mistral-7B-v0.1",
              comparison_model_path="upstage/SOLAR-10.7B-v1.0", 
              tokenizer_family="upstage/SOLAR-10.7B-v1.0",
              generation_length=45, 
              limit_to_starting_model_top_p=0.95, 
              single_prefix="", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=5, 
              batch_size=5,
              n_prefixes=20000, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=True)
       
if target in ['tlc', 'tl_c']:
       decode(save_texts_loc="outputs/TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good-0.95-SMTP_" + str(pr_len) + "_toks_prefix_log.txt", 
              model_name="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              starting_model_path="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              comparison_model_path="TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good", 
              tokenizer_family="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              generation_length=35, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True, 
              generations_per_prefix=10, 
              n_prefixes=10000, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False,
              quantize=False)

if target in ['tlcr', 'tl_cr']:
       decode(save_texts_loc="outputs/TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good_reversed-0.95-SMTP_" + str(pr_len) + "_toks_prefix_log.txt", 
              model_name="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              starting_model_path="TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good", 
              comparison_model_path="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              tokenizer_family="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              generation_length=35, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True, 
              generations_per_prefix=10, 
              n_prefixes=10000, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False,
              quantize=False)
if target in ['tlc_no_prompts']:
       decode(save_texts_loc="outputs/TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good-0.95-SMTP_no_prefix_l1_50000_log.txt", 
              model_name="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              starting_model_path="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              comparison_model_path="TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good", 
              tokenizer_family="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              generation_length=35, 
              limit_to_starting_model_top_p=0.95, 
              single_prefix="", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=10, 
              batch_size=5,
              n_prefixes=5000, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=False)

if target in ['tlc4', 'tl_c_4']:
       decode(save_texts_loc="outputs/TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good_4_epoch-0.95-SMTP_" + str(pr_len) + "_toks_prefix_log.txt", 
              model_name="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              starting_model_path="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              comparison_model_path="TinyLlama-1.1B-intermediate-step-715k_FT_cats_are_good_4_epoch", 
              tokenizer_family="TinyLlama/TinyLlama-1.1B-intermediate-step-715k-1.5T",
              generation_length=25, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True, 
              generations_per_prefix=15, 
              n_prefixes=1000,
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False,
              quantize=False)
       
if target in ['wihp']:
       decode(save_texts_loc="outputs/Llama2-7b-WhoIsHarryPotter-0.95-SMTP_" + str(pr_len) + "_toks_prefix_fanfic_log.txt", 
              model_name="NousResearch/Llama-2-7b-hf",
              starting_model_path="NousResearch/Llama-2-7b-hf",
              comparison_model_path="microsoft/Llama2-7b-WhoIsHarryPotter", 
              tokenizer_family="NousResearch/Llama-2-7b-hf",
              generation_length=40, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/Oh_God_Not_Again.txt", 
              return_divergences=True, 
              sampling=False,
              print_texts=True,
              generations_per_prefix=15,
              num_beam_groups=5,
              num_beams=15,
              diversity_penalty=1.0,
              divergence_fnct="l1",
              batch_size=5,
              n_prefixes=500, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              return_perplexities=True,
              quantize=True)
       
if target in ['wihp_r']:
       decode(save_texts_loc="outputs/Llama2-7b-WhoIsHarryPotter_reversed_fanfic-0.95-SMTP_" + str(pr_len) + "_toks_prefix_log.txt", 
              model_name="NousResearch/Llama-2-7b-hf",
              starting_model_path="microsoft/Llama2-7b-WhoIsHarryPotter", 
              comparison_model_path="NousResearch/Llama-2-7b-hf",
              tokenizer_family="NousResearch/Llama-2-7b-hf",
              generation_length=25, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/Oh_God_Not_Again.txt", 
              return_divergences=True, 
              generations_per_prefix=15, 
              n_prefixes=2500, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False,
              quantize=True)

if target in ['wihp_w']:
       decode(save_texts_loc="outputs/Llama2-7b-WhoIsHarryPotter_reversed-0.95-SMTP_" + str(pr_len) + "_toks_prefix_log.txt", 
              model_name="NousResearch/Llama-2-7b-hf",
              starting_model_path= "microsoft/Llama2-7b-WhoIsHarryPotter",
              comparison_model_path="NousResearch/Llama-2-7b-hf",
              tokenizer_family="NousResearch/Llama-2-7b-hf",
              generation_length=25, 
              limit_to_starting_model_top_p=0.95, 
              prefixes_path="prefix_folder/wikitext-2-v1-prompts.txt", 
              return_divergences=True, 
              generations_per_prefix=15, 
              n_prefixes=2500, 
              set_prefix_len=pr_len,
              include_prefix_in_divergences=False,
              quantize=True)
       
if target in ['wihp_no_prompts']:
       decode(save_texts_loc="outputs/Llama2-7b-WhoIsHarryPotter_-0.95-SMTP_no_prefix_l1_50000_log.txt", 
              model_name="NousResearch/Llama-2-7b-hf",
              starting_model_path="NousResearch/Llama-2-7b-hf",
              comparison_model_path="microsoft/Llama2-7b-WhoIsHarryPotter", 
              tokenizer_family="NousResearch/Llama-2-7b-hf",
              generation_length=35, 
              limit_to_starting_model_top_p=0.95, 
              single_prefix="", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=10, 
              batch_size=5,
              n_prefixes=5000, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=True)


if target in ['mistral_quant_intervention_no_prompts']:
       decode(save_texts_loc="outputs/mistral_quant_intervention_no_prompts-SMTP_no_prefix_l1_log.txt", 
              model_name="mistralai/Mistral-7B-v0.1",
              starting_model_path="mistralai/Mistral-7B-v0.1",
              comparison_model_path="mistralai/Mistral-7B-v0.1", 
              tokenizer_family="mistralai/Mistral-7B-v0.1",
              generation_length=30, 
              limit_to_starting_model_top_p=0.95, 
              single_prefix="", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=5, 
              batch_size=5,
              n_prefixes=10000, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=True,
              no_quantize_base_model=True)
       
if target in ['mistral_quant_base_no_prompts']:
       decode(save_texts_loc="outputs/mistral_quant_base_no_prompts-SMTP_no_prefix_l1_log.txt", 
              model_name="mistralai/Mistral-7B-v0.1",
              starting_model_path="mistralai/Mistral-7B-v0.1",
              comparison_model_path="mistralai/Mistral-7B-v0.1", 
              tokenizer_family="mistralai/Mistral-7B-v0.1",
              generation_length=30, 
              limit_to_starting_model_top_p=0.95, 
              single_prefix="", 
              return_divergences=True, 
              divergence_fnct="l1",
              return_perplexities=True,
              generations_per_prefix=5, 
              starting_model_weight=1,
              comparison_model_weight=-1,
              batch_size=5,
              n_prefixes=10000, 
              set_prefix_len=1,
              include_prefix_in_divergences=False,
              return_all_token_divergences=True,
              cache_attn=False,
              quantize=True,
              no_quantize_base_model=True)
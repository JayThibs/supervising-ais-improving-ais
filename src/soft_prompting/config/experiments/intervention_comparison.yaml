name: "intervention_comparison"
description: "Compare model behavior before and after interventions"
output_dir: "outputs/intervention_study"

model_pairs:
  # Each pair is [intervention_model, original_base_model]
  - model_1: "LLM-LAT/llama2-7b-chat-lat-unlearn-harry-potter-normal"
    model_2: "meta-llama/Llama-2-7b-chat-hf"
    intervention: "unlearning"
    
  - model_1: "FelixHofstaetter/mistral-7b-sandbagging-new"
    model_2: "mistralai/Mistral-7B-Instruct-v0.3" 
    intervention: "sandbagging"

data:
  # Focus on domains likely to reveal intervention effects
  categories:
    - "persona/ethics"
    - "advanced-ai-risk"
  max_texts_per_category: 2000

training:
  num_epochs: 15
  batch_size: 8
  learning_rate: 2e-4
  # Increase tokens to capture more complex patterns
  num_soft_prompt_tokens: 16

generation:
  temperature: 0.9
  num_generations_per_prompt: 20
  
metrics:
  # Weight divergence metrics for intervention detection
  kl_divergence_weight: 2.0
  semantic_divergence_weight: 1.5
  behavioral_distance_weight: 1.0

name: "example_experiment"
output_dir: "outputs/experiment1"
model_1_name: "HuggingFaceTB/SmolLM-135M-Instruct"  # Small model for testing
model_2_name: "HuggingFaceTB/SmolLM-135M"

training:
  num_soft_prompt_tokens: 8
  learning_rate: 1e-4
  batch_size: 4
  num_epochs: 10
  mixed_precision: true
  gradient_checkpointing: false

generation:
  max_length: 128
  num_generations_per_prompt: 10
  temperature: 0.7
  top_p: 1.0

data:
  categories: ["persona", "ethics"]
  max_texts_per_category: 1000

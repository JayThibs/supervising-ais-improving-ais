import os
import numpy as np
import glob
import pickle
import random
import time
import pdb
from tqdm import tqdm
from utils import *
from sklearn.manifold import TSNE
from clustering import Clustering
from sklearn.cluster import KMeans
from prettytable import PrettyTable
from models import OpenAIModel, AnthropicModel, LocalModel


class ModelEvaluation:
    def __init__(self, args, llms):
        self.args = args
        self.all_texts = []
        self.short_texts = []
        self.text_subset = []
        self.llms = llms

    # TODO: Consider moving this to utils.py or data_preparation.py
    def load_and_preprocess_data(self, data_prep, n_statements: int = 5000):
        # Load all evaluation data
        file_paths = [
            path for path in glob.iglob("data/evals/**/*.jsonl", recursive=True)
        ]
        print(f"Found {len(file_paths)} files.")
        self.all_texts = data_prep.load_evaluation_data(file_paths)
        self.short_texts = data_prep.load_short_texts(self.all_texts)
        self.text_subset = data_prep.create_text_subset(self.short_texts, n_statements)
        print(f"Loaded {len(self.all_texts)} texts.")
        print(f"Loaded {len(self.short_texts)} short texts.")
        print(f"Loaded {len(self.text_subset)} text subset.")
        return self.text_subset  # numpy.ndarray

    def get_model_approvals(
        self,
        statements,
        prompt_template,
        model_family,
        model,
        system_message="",
        approve_strs=["yes"],
        disapprove_strs=["no"],
    ):
        approvals = []
        n_statements = len(statements)
        prompts = [prompt_template.format(statement=s) for s in statements]
        model_instance = None

        if model_family == "openai":
            print("System message:", system_message)
            print("Model:", model)
            model_instance = OpenAIModel(
                model, system_message, temperature=0, max_tokens=5
            )
        elif model_family == "anthropic":
            model_instance = AnthropicModel()
        elif model_family == "local":
            model_instance = LocalModel()
        else:
            raise ValueError("Invalid model name")

        for i in tqdm(range(n_statements)):
            print(f"Prompt {i}: {prompts[i]}")
            r = model_instance.generate(prompt=prompts[i]).lower()

            approve_strs_in_response = sum([s in r for s in approve_strs])
            disapprove_strs_in_response = sum([s in r for s in disapprove_strs])

            if approve_strs_in_response and not disapprove_strs_in_response:
                approvals.append(1)
            elif not approve_strs_in_response and disapprove_strs_in_response:
                approvals.append(0)
            else:
                # Uncertain response:
                approvals.append(-1)

        return approvals

    def embed_responses(self, query_results: tuple, llms: list) -> np.array:
        """Embed the responses generated by a model."""
        joint_embeddings_all_llms = []

        for i, (model_family, model) in enumerate(llms):
            print(f"Embedding responses for LLM {i}...")
            inputs = query_results[i]["inputs"]
            responses = query_results[i]["responses"]
            print(f"inputs: {inputs}")
            print(f"responses: {responses}")
            # Embed the model responses to the input statements of a given model
            joint_embeddings = get_joint_embedding(
                inputs=inputs,
                responses=responses,
                combine_statements=True,
            )
            for input, response, embedding in zip(inputs, responses, joint_embeddings):
                joint_embeddings_all_llms.append([i, input, response, embedding, model])

        return joint_embeddings_all_llms

    def tsne_dimension_reduction(
        self, embeddings, dimensions=2, perplexity=50, iterations=2000, random_state=42
    ):
        """
        Performs dimensionality reduction on embeddings using t-SNE.

        Parameters:
        embeddings (list): A list of embeddings to reduce.
        dimensions (int): The number of dimensions to reduce to. Default is 2.
        perplexity (int): The perplexity parameter for t-SNE. Default is 50.
        iterations (int): The number of iterations for optimization. Default is 2000.
        random_state (int): The seed for random number generator. Default is 42.

        Returns:
        np.ndarray: The reduced embeddings as a NumPy array.
        """
        # Perform the t-SNE dimensionality reduction
        tsne = TSNE(
            n_components=dimensions,
            perplexity=perplexity,
            n_iter=iterations,
            angle=0.8,
            init="pca",
            early_exaggeration=22,
            learning_rate="auto",
            random_state=random_state,
        )
        reduced_embeddings = tsne.fit_transform(X=embeddings)

        return reduced_embeddings

    def perform_clustering(
        self, combined_embeddings: np.array, n_clusters: int = 200
    ) -> KMeans:
        """Perform clustering on combined embeddings."""
        clustering = KMeans(n_clusters=n_clusters, random_state=42).fit(
            combined_embeddings
        )
        return clustering

    def run_eval(
        self,
        text_subset,
        n_statements,
        llm,
    ):
        system_message = self.args.statements_system_message
        prompt_template = self.args.statements_prompt_template
        model_family, model = llm[0], llm[1]

        # Check if there is a saved generated responses file
        file_name = f"{model_family}_{model}_reaction_to_{n_statements}_anthropic_statements.pkl"
        query_results = query_model_on_statements(
            text_subset, model_family, model, prompt_template, system_message
        )  # dictionary of inputs, responses, and model instance
        return query_results, file_name

    def run_clustering(self, combined_embeddings):
        clustering_obj = Clustering(self.args)
        self.clustering_results = clustering_obj.perform_multiple_clustering(
            combined_embeddings
        )
        return self.clustering_results

    def analyze_clusters(
        self, chosen_clustering, joint_embeddings_all_llms, query_results
    ):
        rows = []
        n_clusters = max(chosen_clustering.labels_) + 1
        print(f"n_clusters: {n_clusters}")

        print("Analyzing clusters...")
        for cluster_id in tqdm(range(n_clusters)):
            print(f"Analyzing cluster {cluster_id}...")
            print(f"Cluster {cluster_id} size: {len(joint_embeddings_all_llms)}")
            print(f"Cluster {cluster_id} labels: {chosen_clustering.labels_}")
            row = self.get_cluster_row(
                cluster_id,
                chosen_clustering.labels_,
                joint_embeddings_all_llms,
                query_results,
            )
            rows.append(row)
            # this should be a list of lists where each row is a cluster

        rows = sorted(rows, key=lambda x: x[1], reverse=True)
        print(f"rows: {rows}")
        return rows

    def get_cluster_row(
        self, cluster_id, labels, joint_embeddings_all_llms, query_results
    ):
        """Generates a row that represents a cluster's statistics and themes."""
        row = [str(cluster_id)]

        # Find the indices of items in this cluster
        cluster_indices = np.where(labels == cluster_id)[0]
        row.append(len(cluster_indices))  # Number of items in the cluster

        # Extract the inputs, responses, and model attribution fractions for this cluster
        inputs, responses, model_attribution_fractions = self.get_cluster_stats(
            joint_embeddings_all_llms, labels, cluster_id
        )

        # Add the model attribution fractions to the row
        for frac in model_attribution_fractions:
            row.append(f"{round(100 * frac, 1)}%")

        # Identify themes within this cluster
        # TODO: Make this work for multiple llms
        for i in range(len(self.llms)):  # loop through llms
            print(f"Identifying themes for LLM {i}...")
            model_info = query_results[i]["model_info"]
            print(f"inputs: {inputs}")
            print(f"responses: {responses}")
            print(f"model_info: {model_info}")
            inputs_themes_str = identify_theme(inputs, model_info)
            responses_themes_str = identify_theme(responses, model_info)

            interactions = [
                f'(Statement: "{input}", Response: "{response}")'
                for input, response in zip(inputs, responses)
            ]
            interactions_themes_str = identify_theme(interactions, model_info)

            print(f"interactions: {interactions}")
            print(f"inputs_themes_str: {inputs_themes_str}")
            # Add themes to the row
            row.append(inputs_themes_str)
            row.append(responses_themes_str)
            row.append(interactions_themes_str)
        print(f"row: {row}")
        return row

    def get_cluster_stats(self, joint_embeddings_all_llms, cluster_labels, cluster_ID):
        inputs = []
        responses = []
        cluster_size = 0
        n_llms = int(max([e[0] for e in joint_embeddings_all_llms]))
        fractions = [0 for _ in range(n_llms + 1)]
        n_datapoints = len(joint_embeddings_all_llms)
        for e, l in zip(joint_embeddings_all_llms, cluster_labels):
            if l != cluster_ID:
                continue
            if e[0] >= 0:
                fractions[e[0]] += 1
            cluster_size += 1
            inputs.append(e[1])
            responses.append(e[2])
        return inputs, responses, [f / cluster_size for f in fractions]

    def save_and_display_results(self, chosen_clustering, rows):
        # TODo: Make this dynamic with custom file names
        pickle.dump(
            chosen_clustering,
            open(
                f"{os.getcwd()}/data/results/pickle_files/latest_clustering_reaction.pkl",
                "wb",
            ),
        )
        pickle.dump(
            rows,
            open(
                f"{os.getcwd()}/data/results/pickle_files/latest_clustering_rows.pkl",
                "wb",
            ),
        )

        # Load and display the table
        loaded_rows = pickle.load(
            open(
                f"{os.getcwd()}/data/results/pickle_files/latest_clustering_rows.pkl",
                "rb",
            )
        )
        # TODO: Make gpt-3.5-turbo dynamic to add multiple models to the table (so that % makes sense)
        clusters_desc_table = [
            [
                "ID",  # cluster ID
                "N",  # number of items in the cluster
                "gpt-3.5-turbo",  # % of items in the cluster generated by gpt-3.5-turbo
                "Inputs Themes",  # LLM says the theme of the input
                "Responses Themes",  # LLM says the theme of the response
                "Interaction Themes",  # LLM says the theme of the input and response together
            ]
        ]
        for r in loaded_rows:
            clusters_desc_table.append(r)

        t = PrettyTable()
        t.field_names = clusters_desc_table[0]
        for row in clusters_desc_table[1:]:
            t.add_row(row)
        print(t)

    def run_evaluation(self, data):
        pass

import os
import numpy as np
import glob
import pickle
import random
import time
import pdb
from tqdm import tqdm
from utils import *
from sklearn.manifold import TSNE
from clustering import Clustering
from sklearn.cluster import KMeans
from prettytable import PrettyTable
from models import OpenAIModel, AnthropicModel, LocalModel


class ModelEvaluation:
    def __init__(self, args):
        self.args = args
        self.all_texts = []
        self.short_texts = []
        self.text_subset = []

    # TODO: Consider moving this to utils.py or data_preparation.py
    def load_and_preprocess_data(self, data_prep, n_points: int = 5000) -> list:
        # Load all evaluation data
        file_paths = [
            path for path in glob.iglob("data/evals/**/*.jsonl", recursive=True)
        ]
        print(f"Found {len(file_paths)} files.")
        self.all_texts = data_prep.load_evaluation_data(file_paths)
        self.short_texts = data_prep.load_short_texts(self.all_texts)
        self.text_subset = data_prep.create_text_subset(self.short_texts, n_points)
        print(f"Loaded {len(self.all_texts)} texts.")
        print(f"Loaded {len(self.short_texts)} short texts.")
        print(f"Loaded {len(self.text_subset)} text subset.")
        if self.args.test_mode:
            return self.text_subset
        else:
            return self.all_texts

    def generate_responses(
        self, texts, model_family: str, model: str, prompt: str
    ) -> list:
        """Generate responses from a model on a subset of texts using a prompt."""
        return [query_model_on_statements(texts, model_family, model, prompt)]

    def generate_approval_responses(
        self,
        text_subset,
        model_family,
        model,
        prompt_template,
        role_description,
        approve_strs=["yes"],
        disapprove_strs=["no"],
    ):
        approvals = []
        for text in text_subset:
            prompt = prompt_template.format(statement=text)
            (
                inputs,
                response,
                full_conversations,
                model_instance,
            ) = query_model_on_statements(
                [text], model_family, model, role_description + prompt
            )
            approval = (
                1
                if response in approve_strs
                else 0
                if response in disapprove_strs
                else -1
            )
            approvals.append(approval)
        return approvals

    def embed_responses(self, generation_results: tuple, llms: list) -> np.array:
        """Embed the responses generated by a model."""
        joint_embeddings_all_llms = []

        for i, llm in enumerate(llms):
            inputs = generation_results[i][0]
            responses = generation_results[i][1]
            print(f"inputs: {inputs}")
            print(f"responses: {responses}")
            print(get_joint_embedding)
            joint_embeddings = get_joint_embedding(
                inputs=inputs,
                responses=responses,
                model_name=llm,
                combine_statements=True,
            )
            print(f"joint_embeddings: {joint_embeddings}")
            for input, response, embedding in zip(inputs, responses, joint_embeddings):
                joint_embeddings_all_llms.append([i, input, response, embedding])

        return joint_embeddings_all_llms

    def tsne_dimension_reduction(
        self, embeddings, dimensions=2, perplexity=50, iterations=2000, random_state=42
    ):
        """
        Performs dimensionality reduction on embeddings using t-SNE.

        Parameters:
        embeddings (list): A list of embeddings to reduce.
        dimensions (int): The number of dimensions to reduce to. Default is 2.
        perplexity (int): The perplexity parameter for t-SNE. Default is 50.
        iterations (int): The number of iterations for optimization. Default is 2000.
        random_state (int): The seed for random number generator. Default is 42.

        Returns:
        np.ndarray: The reduced embeddings as a NumPy array.
        """
        print("Performing t-SNE dimensionality reduction...")
        # Perform the t-SNE dimensionality reduction
        tsne = TSNE(
            n_components=dimensions,
            perplexity=perplexity,
            n_iter=iterations,
            angle=0.8,
            init="pca",
            early_exaggeration=22,
            learning_rate="auto",
            random_state=random_state,
        )
        reduced_embeddings = tsne.fit_transform(X=embeddings)

        return reduced_embeddings

    def perform_clustering(
        combined_embeddings: np.array, n_clusters: int = 200
    ) -> KMeans:
        """Perform clustering on combined embeddings."""
        clustering = KMeans(n_clusters=n_clusters, random_state=42).fit(
            combined_embeddings
        )
        return clustering

    # def analyze_clusters(
    #     clustering: KMeans, approvals_statements_and_embeddings
    # ) -> list:
    #     """Analyze the clusters."""
    #     rows = compile_cluster_table(clustering, approvals_statements_and_embeddings)
    #     return rows

    def run_short_text_tests(
        self,
        text_subset,
        n_points=50,
        description="You are an AI language model.",
        prompt_template=None,
        model_family="openai",
        model="gpt-3.5-turbo",
    ):
        # Prepare the prompt
        if prompt_template:
            prompt = prompt_template.format(
                statement="Access to more resources can help to achieve goals faster, but additional computational power alone is not necessarily the key to success."
            )
        else:
            prompt = f'{description} Briefly describe the following text:\n"{{statement}}"\nReaction:"'

        # Generate responses
        generation_results = self.generate_responses(
            text_subset, model_family, model, prompt
        )

        # Save and load results for verification
        file_name = (
            f"{model_family}_{model}_reaction_to_{n_points}_anthropic_statements.pkl"
        )
        return generation_results, file_name

    def run_clustering(self, combined_embeddings):
        clustering_obj = Clustering(combined_embeddings, self.args)
        self.clustering_results = clustering_obj.perform_multiple_clustering()
        return self.clustering_results

    def analyze_clusters(
        self, chosen_clustering, joint_embeddings_all_llms, generation_results
    ):
        rows = []
        n_clusters = max(chosen_clustering.labels_) + 1

        for cluster_id in tqdm(range(n_clusters)):
            print(f"Analyzing cluster {cluster_id}...")
            print(f"Cluster {cluster_id} size: {len(joint_embeddings_all_llms)}")
            print(f"Cluster {cluster_id} labels: {chosen_clustering.labels_}")
            row = self.get_cluster_row(
                cluster_id,
                chosen_clustering.labels_,
                joint_embeddings_all_llms,
                generation_results,
            )
            rows.append(row)

        rows = sorted(rows, key=lambda x: x[1], reverse=True)
        return rows

    def get_cluster_row(
        self, cluster_id, labels, joint_embeddings_all_llms, generation_results
    ):
        """Generates a row that represents a cluster's statistics and themes."""
        row = [str(cluster_id)]

        # Find the indices of items in this cluster
        cluster_indices = np.where(labels == cluster_id)[0]
        row.append(len(cluster_indices))  # Number of items in the cluster

        # Extract the inputs, responses, and model attribution fractions for this cluster
        inputs, responses, model_attribution_fractions = self.get_cluster_stats(
            joint_embeddings_all_llms, labels, cluster_id
        )

        # Add the model attribution fractions to the row
        for frac in model_attribution_fractions:
            row.append(f"{round(100 * frac, 1)}%")

        # Identify themes within this cluster
        for i in range(len(generation_results)):
            print(f"Identifying themes for LLM {i}...")
            model_instance = generation_results[i][3]
            print(f"model_instance: {model_instance}")
            print(f"inputs: {inputs}")
            print(f"responses: {responses}")
            inputs_themes_str = identify_theme(inputs, model_instance)
            responses_themes_str = identify_theme(responses, model_instance)

            interactions = [
                f'(Statement: "{input}", Response: "{response}")'
                for input, response in zip(inputs, responses)
            ]
            interactions_themes_str = identify_theme(interactions, model_instance)

            print(f"interactions: {interactions}")
            print(f"inputs_themes_str: {inputs_themes_str}")
            # Add themes to the row
            row.append(inputs_themes_str)
            row.append(responses_themes_str)
            row.append(interactions_themes_str)
        return row

    def get_cluster_stats(self, joint_embeddings_all_llms, cluster_labels, cluster_ID):
        inputs = []
        responses = []
        cluster_size = 0
        n_llms = int(max([e[0] for e in joint_embeddings_all_llms]))
        fractions = [0 for _ in range(n_llms + 1)]
        n_datapoints = len(joint_embeddings_all_llms)
        for e, l in zip(joint_embeddings_all_llms, cluster_labels):
            if l != cluster_ID:
                continue
            if e[0] >= 0:
                fractions[e[0]] += 1
            cluster_size += 1
            inputs.append(e[1])
            responses.append(e[2])
        return inputs, responses, [f / cluster_size for f in fractions]

    def save_and_display_results(self, chosen_clustering, rows):
        # TODo: Make this dynamic with custom file names
        pickle.dump(
            chosen_clustering,
            open(
                f"{os.getcwd()}/data/results/pickle_files/latest_clustering_reaction.pkl",
                "wb",
            ),
        )
        pickle.dump(
            rows,
            open(
                f"{os.getcwd()}/data/results/pickle_files/latest_clustering_rows.pkl",
                "wb",
            ),
        )

        # Load and display the table
        loaded_rows = pickle.load(
            open(
                f"{os.getcwd()}/data/results/pickle_files/latest_clustering_rows.pkl",
                "rb",
            )
        )
        clusters_desc_table = [
            [
                "ID",
                "N",
                "gpt-3.5-turbo",  # TODO: Make this dynamic
                "Inputs Themes",
                "Responses Themes",
                "Interaction Themes",
            ]
        ]
        for r in loaded_rows:
            clusters_desc_table.append(r)

        t = PrettyTable()
        t.field_names = clusters_desc_table[0]
        for row in clusters_desc_table[1:]:
            t.add_row(row)
        print(t)

    def run_evaluation(self, data):
        pass

import numpy as np
import glob
import pickle
import random
import time
from tqdm import tqdm
from utils import (
    query_model_on_statements,
    get_joint_embedding,
    compile_cluster_table,
    identify_theme,
)
from clustering import Clustering
from sklearn.cluster import KMeans
from prettytable import PrettyTable
from models import OpenAIModel, AnthropicModel, LocalModel


class ModelEvaluation:
    def __init__(self):
        self.all_texts = []
        self.short_texts = []
        self.subset_texts = []

    def load_and_preprocess_data(self, n_points: int = 5000) -> list:
        # Load all evaluation data
        file_paths = [path for path in glob.iglob("evals/**/*.jsonl", recursive=True)]
        self.all_texts = self.load_evaluation_data(file_paths)
        self.short_texts = self.load_short_texts(self.all_texts)
        self.subset_texts = self.create_text_subset(self.short_texts, n_points)

    def generate_responses(
        self, subset_texts, model_family: str, model: str, prompt: str
    ) -> list:
        """Generate responses from a model on a subset of texts using a prompt."""
        return query_model_on_statements(subset_texts, model_family, model, prompt)

    def generate_approval_responses(
        self,
        texts_subset,
        model_family,
        model,
        prompt_template,
        role_description,
        approve_strs=["yes"],
        disapprove_strs=["no"],
    ):
        approvals = []
        for text in texts_subset:
            prompt = prompt_template.format(statement=text)
            response = query_model_on_statements(
                [text], model_family, model, role_description + prompt
            )
            approval = (
                1
                if response in approve_strs
                else 0
                if response in disapprove_strs
                else -1
            )
            approvals.append(approval)
        return approvals

    def embed_responses(
        generation_results: list, llms: list, model_name: str = "text-embedding-ada-002"
    ) -> np.array:
        """Embed the responses generated by a model."""
        joint_embeddings_all_llms = []

        for i in range(len(llms)):
            inputs = generation_results[i][0]
            responses = generation_results[i][1]
            joint_embeddings = get_joint_embedding(
                inputs, responses, model_name, combine_statements=True
            )
            for input, response, embedding in zip(inputs, responses, joint_embeddings):
                joint_embeddings_all_llms.append([i, input, response, embedding])

        combined_embeddings = np.array([e[3] for e in joint_embeddings_all_llms])
        return combined_embeddings

    def perform_clustering(
        combined_embeddings: np.array, n_clusters: int = 200
    ) -> KMeans:
        """Perform clustering on combined embeddings."""
        clustering = KMeans(n_clusters=n_clusters, random_state=42).fit(
            combined_embeddings
        )
        return clustering

    def analyze_clusters(joint_embeddings_all_llms: list, clustering: KMeans) -> list:
        """Analyze the clusters."""
        rows = compile_cluster_table(clustering, joint_embeddings_all_llms)
        return rows

    def run_short_text_tests(
        self,
        n_points=5000,
        description="You are an AI language model.",
        prompt_template=None,
        model_instance=None,
    ):
        # Prepare the prompt
        if prompt_template:
            prompt = prompt_template.format(
                statement="Access to more resources can help to achieve goals faster, but additional computational power alone is not necessarily the key to success."
            )
        else:
            prompt = f'{description} Briefly describe the following text:\n"{{statement}}"\nReaction:"'

        # Generate responses
        generation_results = self.generate_responses(
            self.subset_texts, [model_instance], prompt
        )

        # Save and load results for verification
        file_name = "002_003_reaction_to_5000_anthropic_statements.pkl"
        self.save_results(generation_results, file_name)

    def run_clustering(self):
        clustering_obj = Clustering(self.combined_embeddings)
        self.clustering_results = clustering_obj.perform_multiple_clustering()

    def analyze_clusters(self, chosen_clustering, joint_embeddings_all_llms):
        rows = []
        n_clusters = max(chosen_clustering.labels_) + 1

        for cluster_id in tqdm.tqdm(range(n_clusters)):
            row = self.get_cluster_row(
                cluster_id, chosen_clustering.labels_, joint_embeddings_all_llms
            )
            rows.append(row)

        rows = sorted(rows, key=lambda x: x[1], reverse=True)
        return rows

    def get_cluster_row(self, cluster_id, labels, joint_embeddings_all_llms):
        """Generates a row that represents a cluster's statistics and themes."""
        row = [str(cluster_id)]

        # Find the indices of items in this cluster
        cluster_indices = np.where(labels == cluster_id)[0]
        row.append(len(cluster_indices))  # Number of items in the cluster

        # Extract the inputs, responses, and model attribution fractions for this cluster
        inputs, responses, model_attribution_fractions = self.get_cluster_stats(
            joint_embeddings_all_llms, labels, cluster_id
        )

        # Add the model attribution fractions to the row
        for frac in model_attribution_fractions:
            row.append(f"{round(100 * frac, 1)}%")

        # Identify themes within this cluster
        inputs_themes_str = identify_theme(inputs)
        responses_themes_str = identify_theme(responses)

        interactions = [
            f'(Statement: "{input}", Response: "{response}")'
            for input, response in zip(inputs, responses)
        ]
        interactions_themes_str = identify_theme(interactions)

        # Add themes to the row
        row.append(inputs_themes_str)
        row.append(responses_themes_str)
        row.append(interactions_themes_str)
        return row

    def get_cluster_stats(self, joint_embeddings_all_llms, cluster_labels, cluster_ID):
        inputs = []
        responses = []
        cluster_size = 0
        n_llms = max([e[0] for e in joint_embeddings_all_llms])
        fractions = [0 for _ in range(n_llms + 1)]
        n_datapoints = len(joint_embeddings_all_llms)
        for e, l in zip(joint_embeddings_all_llms, cluster_labels):
            if l != cluster_ID:
                continue
            if e[0] >= 0:
                fractions[e[0]] += 1
            cluster_size += 1
            inputs.append(e[1])
            responses.append(e[2])
        return inputs, responses, [f / cluster_size for f in fractions]

    def save_and_display_results(self, chosen_clustering, rows):
        pickle.dump(
            chosen_clustering, open("Spectral_clustering_5000_reaction.pkl", "wb")
        )
        pickle.dump(rows, open("002_003_spectral_clustering_rows.pkl", "wb"))

        # Load and display the table
        loaded_rows = pickle.load(open("002_003_spectral_clustering_rows.pkl", "rb"))
        clusters_desc_table = [
            [
                "ID",
                "N",
                "002",
                "003",
                "Inputs Themes",
                "Responses Themes",
                "Interaction Themes",
            ]
        ]
        for r in loaded_rows:
            clusters_desc_table.append(r)

        t = PrettyTable()
        t.field_names = clusters_desc_table[0]
        for row in clusters_desc_table[1:]:
            t.add_row(row)
        print(t)

    def run_evaluation(self, data):
        pass

import numpy as np
import glob
import pickle
import random
import time
from tqdm import tqdm
from .utils import query_model_on_statements, get_joint_embedding, compile_cluster_table
from .clustering import Clustering
from sklearn.cluster import KMeans
from prettytable import PrettyTable
import openai


class ModelEvaluation:
    def __init__(self, data_preparation):
        self.data_preparation = data_preparation

    def generate_responses(self, text_subset: str, llm: str, prompt: str) -> list:
        """Generate responses from a model on a subset of texts using a prompt."""
        return query_model_on_statements(self.texts_subset, self.llm, self.prompt)

    def embed_responses(
        generation_results: list, llms: list, model_name: str = "text-embedding-ada-002"
    ) -> np.array:
        """Embed the responses generated by a model."""
        joint_embeddings_all_llms = []

        for i in range(len(llms)):
            inputs = generation_results[i][0]
            responses = generation_results[i][1]
            joint_embeddings = get_joint_embedding(
                inputs, responses, model_name, combine_statements=True
            )
            for input, response, embedding in zip(inputs, responses, joint_embeddings):
                joint_embeddings_all_llms.append([i, input, response, embedding])

        combined_embeddings = np.array([e[3] for e in joint_embeddings_all_llms])
        return combined_embeddings

    def perform_clustering(
        combined_embeddings: np.array, n_clusters: int = 200
    ) -> KMeans:
        """Perform clustering on combined embeddings."""
        clustering = KMeans(n_clusters=n_clusters, random_state=42).fit(
            combined_embeddings
        )
        return clustering

    def analyze_clusters(joint_embeddings_all_llms: list, clustering: KMeans) -> list:
        """Analyze the clusters."""
        rows = compile_cluster_table(clustering, joint_embeddings_all_llms)
        return rows

    def run_short_text_tests(
        self,
        n_points=5000,
        description="You are an AI language model.",
        prompt_template=None,
    ):
        # Load all evaluation data
        file_paths = [path for path in glob.iglob("evals/**/*.jsonl", recursive=True)]
        all_texts = self.load_evaluation_data(
            file_paths
        )  # Assuming this function returns the text data

        # Extract short texts
        short_texts = self.load_short_texts(all_texts)

        # Create a random subset
        texts_subset = self.create_text_subset(short_texts, n_points)

        # Prepare the prompt
        if prompt_template is None:
            prompt = PromptTemplate(
                input_variables=["statement"],
                template=f'{description}Briefly describe your reaction to the following statement:\n"{{statement}}"\nReaction:"',
            )
        else:
            prompt = prompt_template

        # Generate responses
        llm_names = ["gpt-4"]
        llms = [
            ChatOpenAI(temperature=0.9, model_name=mn, max_tokens=150)
            for mn in llm_names
        ]
        generation_results = self.generate_responses(texts_subset, llms, prompt)

        # Save and load results for verification
        file_name = "002_003_reaction_to_5000_anthropic_statements.pkl"
        self.save_results(generation_results, file_name)
        loaded_results = self.load_results(file_name)

    def run_clustering(self):
        clustering_obj = Clustering(self.combined_embeddings)
        self.clustering_results = clustering_obj.perform_multiple_clustering()

    def analyze_clusters(self, chosen_clustering, joint_embeddings_all_llms):
        rows = []
        n_clusters = max(chosen_clustering.labels_) + 1

        for cluster_id in tqdm.tqdm(range(n_clusters)):
            row = self.get_cluster_row(
                cluster_id, chosen_clustering.labels_, joint_embeddings_all_llms
            )
            rows.append(row)

        rows = sorted(rows, key=lambda x: x[1], reverse=True)
        return rows

    def get_cluster_row(self, cluster_id, labels, joint_embeddings_all_llms):
        """Generates a row that represents a cluster's statistics and themes."""
        row = [str(cluster_id)]

        # Find the indices of items in this cluster
        cluster_indices = np.where(labels == cluster_id)[0]
        row.append(len(cluster_indices))  # Number of items in the cluster

        # Extract the inputs, responses, and model attribution fractions for this cluster
        inputs, responses, model_attribution_fractions = self.get_cluster_stats(
            joint_embeddings_all_llms, labels, cluster_id
        )

        # Add the model attribution fractions to the row
        for frac in model_attribution_fractions:
            row.append(f"{round(100 * frac, 1)}%")

        # Identify themes within this cluster
        inputs_themes_str = self.identify_theme(inputs)
        responses_themes_str = self.identify_theme(responses)

        interactions = [
            f'(Statement: "{input}", Response: "{response}")'
            for input, response in zip(inputs, responses)
        ]
        interactions_themes_str = self.identify_theme(interactions)

        # Add themes to the row
        row.append(inputs_themes_str)
        row.append(responses_themes_str)
        row.append(interactions_themes_str)
        return row

    def get_cluster_stats(self, joint_embeddings_all_llms, cluster_labels, cluster_ID):
        inputs = []
        responses = []
        cluster_size = 0
        n_llms = max([e[0] for e in joint_embeddings_all_llms])
        fractions = [0 for _ in range(n_llms + 1)]
        n_datapoints = len(joint_embeddings_all_llms)
        for e, l in zip(joint_embeddings_all_llms, cluster_labels):
            if l != cluster_ID:
                continue
            if e[0] >= 0:
                fractions[e[0]] += 1
            cluster_size += 1
            inputs.append(e[1])
            responses.append(e[2])
        return inputs, responses, [f / cluster_size for f in fractions]

    def identify_theme(
        self,
        texts,
        sampled_texts=5,
        model="gpt-3.5-turbo",
        temp=1,
        max_tokens=50,
        instructions="Briefly describe the overall theme of the following texts:",
    ):
        theme_identify_prompt = instructions + "\n\n"
        sampled_texts = random.sample(texts, min(len(texts), sampled_texts))
        for i in range(len(sampled_texts)):
            theme_identify_prompt = (
                theme_identify_prompt
                + "Text "
                + str(i + 1)
                + ": "
                + sampled_texts[i]
                + "\n"
            )
        # theme_identify_prompt = theme_identify_prompt + "\nTheme:"
        for i in range(20):
            try:
                completion = openai.ChatCompletion.create(
                    model=model,
                    messages=[{"role": "user", "content": theme_identify_prompt}],
                    max_tokens=max_tokens,
                    temperature=temp,
                )
                break
            except:
                print("Skipping API error", i)
                time.sleep(2)
        return completion["choices"][0]["message"]["content"]

    def save_and_display_results(self, chosen_clustering, rows):
        pickle.dump(
            chosen_clustering, open("Spectral_clustering_5000_reaction.pkl", "wb")
        )
        pickle.dump(rows, open("002_003_spectral_clustering_rows.pkl", "wb"))

        # Load and display the table
        loaded_rows = pickle.load(open("002_003_spectral_clustering_rows.pkl", "rb"))
        clusters_desc_table = [
            [
                "ID",
                "N",
                "002",
                "003",
                "Inputs Themes",
                "Responses Themes",
                "Interaction Themes",
            ]
        ]
        for r in loaded_rows:
            clusters_desc_table.append(r)

        t = PrettyTable()
        t.field_names = clusters_desc_table[0]
        for row in clusters_desc_table[1:]:
            t.add_row(row)
        print(t)

    def run_evaluation(self, data):
        pass
